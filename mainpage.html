<!DOCTYPE html>
<html>
<head>
<APM_DO_NOT_TOUCH>

<script type="text/javascript">
(function(){
window.ePy=!!window.ePy;try{(function(){(function JI(){var l=!1;function O(l){for(var O=0;l--;)O+=s(document.documentElement,null);return O}function s(l,O){var _="vi";O=O||new S;return LI(l,function(l){l.setAttribute("data-"+_,O.l$());return s(l,O)},null)}function S(){this.il=1;this.lJ=0;this.j_=this.il;this._I=null;this.l$=function(){this._I=this.lJ+this.j_;if(!isFinite(this._I))return this.reset(),this.l$();this.lJ=this.j_;this.j_=this._I;this._I=null;return this.j_};this.reset=function(){this.il++;this.lJ=0;this.j_=this.il}}var _=!1;
function II(l,O){var s=document.createElement(l);O=O||document.body;O.appendChild(s);s&&s.style&&(s.style.display="none")}function jI(O,s){s=s||O;var S="|";function II(l){l=l.split(S);var O=[];for(var s=0;s<l.length;++s){var _="",jI=l[s].split(",");for(var oI=0;oI<jI.length;++oI)_+=jI[oI][oI];O.push(_)}return O}var jI=0,LI="datalist,details,embed,figure,hrimg,strong,article,formaddress|audio,blockquote,area,source,input|canvas,form,link,tbase,option,details,article";LI.split(S);LI=II(LI);LI=new RegExp(LI.join(S),
"g");while(LI.exec(O))LI=new RegExp((""+new Date)[8],"g"),l&&(_=!0),++jI;return s(jI&&1)}function LI(l,O,s){(s=s||_)&&II("div",l);l=l.children;var S=0;for(var jI in l){s=l[jI];try{s instanceof HTMLElement&&(O(s),++S)}catch(LI){}}return S}jI(JI,O)})();var lI=51;try{var OI,ZI,_I=J(598)?1:0,Ij=J(271)?1:0,lj=J(956)?0:1,Lj=J(787)?1:0;for(var sj=(J(148),0);sj<ZI;++sj)_I+=(J(207),2),Ij+=J(966)?1:2,lj+=J(414)?2:1,Lj+=J(555)?3:2;OI=_I+Ij+lj+Lj;window.I_===OI&&(window.I_=++OI)}catch(Sj){window.I_=OI}
var _j=!0;function L(I,l){I+=l;return I.toString(36)}function IJ(I){var l=41;!I||document[z(l,159,146,156,146,139,146,149,146,157,162,124,157,138,157,142)]&&document[z(l,159,146,156,146,139,146,149,146,157,162,124,157,138,157,142)]!==L(68616527625,l)||(_j=!1);return _j}function Z(I){var l=arguments.length,O=[],s=1;while(s<l)O[s-1]=arguments[s++]-I;return String.fromCharCode.apply(String,O)}function jJ(){}IJ(window[jJ[Z(lI,161,148,160,152)]]===jJ);IJ(typeof ie9rgb4!==L(1242178186148,lI));
IJ(RegExp("\x3c")[L(1372154,lI)](function(){return"\x3c"})&!RegExp(L(42838,lI))[L(1372154,lI)](function(){return"'x3'+'d';"}));
var lJ=window[Z(lI,148,167,167,148,150,155,120,169,152,161,167)]||RegExp(z(lI,160,162,149,156,175,148,161,151,165,162,156,151),Z(lI,156))[L(1372154,lI)](window["\x6e\x61vi\x67a\x74\x6f\x72"]["\x75\x73e\x72A\x67\x65\x6et"]),LJ=+new Date+(J(214)?6E5:596386),OJ,sJ,SJ,iJ=window[Z(lI,166,152,167,135,156,160,152,162,168,167)],Il=lJ?J(705)?3E4:40892:J(876)?4891:6E3;
document[z(lI,148,151,151,120,169,152,161,167,127,156,166,167,152,161,152,165)]&&document[z(lI,148,151,151,120,169,152,161,167,127,156,166,167,152,161,152,165)](z(lI,169,156,166,156,149,156,159,156,167,172,150,155,148,161,154,152),function(I){var l=1;document[z(l,119,106,116,106,99,106,109,106,117,122,84,117,98,117,102)]&&(document[Z(l,119,106,116,106,99,106,109,106,117,122,84,117,98,117,102)]===L(1058781982,l)&&I[Z(l,106,116,85,115,118,116,117,102,101)]?SJ=!0:document[Z(l,119,106,116,106,99,106,
109,106,117,122,84,117,98,117,102)]===L(68616527665,l)&&(OJ=+new Date,SJ=!1,jl()))});function jl(){if(!document[z(57,170,174,158,171,178,140,158,165,158,156,173,168,171)])return!0;var I=+new Date;if(I>LJ&&(J(902)?723376:6E5)>I-OJ)return IJ(!1);var l=IJ(sJ&&!SJ&&OJ+Il<I);OJ=I;sJ||(sJ=!0,iJ(function(){sJ=!1},J(179)?1:0));return l}jl();var Ll=[J(590)?17795081:20994727,J(465)?27611931586:2147483647,J(334)?1558153217:920579028];
function ol(I){var l=98;I=typeof I===L(1743045578,l)?I:I[z(l,214,209,181,214,212,203,208,201)](J(745)?36:18);var O=window[I];if(!O||!O[z(l,214,209,181,214,212,203,208,201)])return;var s=""+O;window[I]=function(I,l){sJ=!1;return O(I,l)};window[I][Z(l,214,209,181,214,212,203,208,201)]=function(){return s}}for(var Ol=(J(614),0);Ol<Ll[L(1294399154,lI)];++Ol)ol(Ll[Ol]);IJ(!1!==window[z(lI,152,131,172)]);window.lS=window.lS||{};window.lS.Zl="08bb46f3871940007e6e8f47343cb314e1762482dc73099acbe8781a5be5f0b41a0ec03831f668c987a54933dc8491f3884cc195768ce49d4d7df7e7b1ad2ac7e7f4f9d06af59d73";
function z(I){var l=arguments.length,O=[];for(var s=1;s<l;++s)O.push(arguments[s]-I);return String.fromCharCode.apply(String,O)}function zl(I){var l=+new Date,O;!document[Z(72,185,189,173,186,193,155,173,180,173,171,188,183,186,137,180,180)]||l>LJ&&(J(425)?6E5:864253)>l-OJ?O=IJ(!1):(O=IJ(sJ&&!SJ&&OJ+Il<l),OJ=l,sJ||(sJ=!0,iJ(function(){sJ=!1},J(500)?1:0)));return!(arguments[I]^O)}function J(I){return 792>I}(function Zl(l){return l?0:Zl(l)*Zl(l)})(!0);})();}catch(x){}finally{ie9rgb4=void(0);};function ie9rgb4(a,b){return a>>b>>0};

})();

</script>
</APM_DO_NOT_TOUCH>

<script type="text/javascript" src="/TSPD/082149a1b4ab2000560f8714737119f7a350274e466786398a7ebf590b5ed42a1e42b75127d120e0?type=9"></script>

    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="We propose a new method for weight space learning which trains a Deep Linear Probe Generator to analyze neural networks">
    <meta property="og:title" content="Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration"/>
    <meta property="og:description" content="We propose a new method for weight space learning which trains a Deep Linear Probe Generator to analyze neural networks"/>
    <meta property="og:url" content="https://vision.huji.ac.il/probegen/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/probegen_banner.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="500"/>


    <meta name="twitter:title" content="Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration">
    <meta name="twitter:description" content="We propose a new method for weight space learning which trains a Deep Linear Probe Generator to analyze neural networks">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="weight space learning, ProbeGen, Probing">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                <a href="" target="_blank">Author1</a>,</span>
                        <span class="author-block">
                  <a href="" target="_blank">Author2</a>,</span>
                        <span class="author-block">
                    <a href="" target="_blank">Author3</a>,</span>
                        <span class="author-block">
                    <a href="" target="_blank">Author4</a>
                  </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">Chongqing University of Posts and Telecommunications</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                        <a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf" target="_blank"
                           class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- Github link -->
                            <span class="link-block">
                    <a href="" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                  <a href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Teaser Image-->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="static/image/syn1.png" alt="An overview of our method"/>
            <h2 class="subtitle has-text-centered">
            We propose a unified framework for multi-weather nighttime image restoration that disentangles lighting and texture via Retinex-based priors, 
            while capture feature representations of diverse weather effects using dynamic specificity-commonality synergy.
            </h2>
        </div>
    </div>
</section>
<!-- End teaser Image -->


<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Restoring nighttime images affected by multiple adverse weather conditions is a practical yet under-explored research problem, as multiple weather conditions often coexist in the real world alongside various lighting effects at night. 
                        This paper first explores the challenging multi-weather nighttime image restoration task, where various types of weather degradations are intertwined with flare effects.
                        To support the research, we contribute AllWeatherNight dataset, featuring 10K high-quality nighttime images with diverse compositional degradations (e.g., haze, rain streak, raindrop, snow and flare).
                        We also present a unified nighttime image restoration framework ClearNight, which effectively removes complex degradations in one go. 
                        Specifically, ClearNight extracts Retinex-based dual priors and explicitly guides the network to learn disentangled illumination and intrinsic texture contents, thereby enhancing restoration effectiveness in nighttime scenarios. 
                        To better represent the common and unique characters of different adverse weather conditions, we propose a dynamic specific-commonality synergy method, which adaptively selects optimal candidate blocks for different weather degradations.
                        ClearNight achieves state-of-the-art performance on both synthetic and real-world images.
                        Comprehensive analysis and ablation experiments validate the effectiveness of our dataset and proposed method.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-full">
                <div class="content">
                    <div class="level-set has-text-justified">
                        <p>
                            We observe that uneven lighting conditions in real-world nighttime scenes often interact with weather degradations.
                            To synthesize more realistic nighttime images with adverse weather conditions, we introduce an illumination-aware degradation generation approach. 
                            To account for this, we derive Retinex decomposition~\cite{retinex1} to extract illumination maps as weights for subsequent weather degradation synthesis.
<!--                         <ol>
                            <li>Comparing a vanilla probing baseline to previous graph based and mechanistic approaches. With enough probes: (a) vanilla probing performs better than graph approaches
                                that does not use probing. (b) Graph approaches become equivalent to probing only when they also use probing features.
                            </li>
                            <li>Comparing learned probes and probes from randomly selected data. We show that synthetic probes are equally effective as latent optimized ones.</li>
                        </ol> -->
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
    </div>
</section>


<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/image/real1.png" alt="A table comparing probing to previous approaches"/>
                    <h2 class="subtitle has-text-centered">
                        <em><b>Vanilla Probing vs. Other Approaches.</b></em> Comparing a vanilla probing approach with previous
                        graph based and mechanistic approaches (numbers of probes in brackets).
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/synthetic_data_space.png" alt="Comparing synthetic data to learnt probes"/>
                    <h2 class="subtitle has-text-centered">
                        <em><b>Latent Optimized Probes vs. Synthetic Data as Probes.</b></em> Comparing learned probes and probes from randomly selected data.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-full">
                <div class="content">
                    <div class="level-set has-text-justified">
                        <p>
                            We propose <em>Deep Linear Probe Generators</em> (<strong>ProbeGen</strong>) for learning better probes. ProbeGen optimizes a
                            deep generator module limited to linear expressivity, that shares information between the
                            different probes. It then observes the responses from all probes, and trains an MLP classifier on
                            them. While simple, we demonstrate it greatly enhances probing methods, and also outperforms
                            other approaches by a large margin.
                        </p>
                    </div>
                </div>

            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="static/images/ProbeGen_results.png" alt="Main results of our method ProbeGen"/>
        </div>
    </div>
</section>


<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-full">
                <div class="content">
                    <div class="level-set has-text-justified">
                        <p>
                            ProbeGen represents each model as an ordered list of output
                            values based on carefully chosen probes. These representations often have semantic meanings as
                            the output space of the model (here, image pixels or logits) are semantic by design.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section hero is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/mnist_queries.png" alt="MNIST INR Representation visualization"/>
                    <h2 class="subtitle has-text-centered">
                        <em><b>MNIST INR Representations.</b></em> ProbeGen chooses object centric locations as suitable for this task,
                        while Vanilla Probing chooses locations scattered around the image, including pixels far out of the image.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/cifar_queries.png" alt="CIFAR10 Wild Park Representation visualization"/>
                    <h2 class="subtitle has-text-centered">
                        <em><b>CIFAR10 Wild Park Representations.</b></em> The values become more uniform as the accuracy of the models
                        decreases, and sharper as it increases. This suggests that ProbeGen uses some form of prediction
                        entropy in its classifier. We validate this by training a classifier that only takes the
                        entropy of each probe as its features, which already reaches a Kendallג€™s ֿ„ of 0.877.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/probes_comp_flat_space.png" alt="Comparing the probes learned from different algorithms"/>
                    <h2 class="subtitle has-text-centered">
                        <em><b>ProbeGen vs. Vanilla Probing Learned Probes.</b></em> Although both not interpetable by humans,
                        it is clear that ProbeGen probes have much more structure than latent-optimized ones.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@misc{kahana2024deeplinearprobegenerators,
      title={Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration},
      author={Author1 and Author2 and Author3 and Author4},
      year={2025},
      eprint={xxx.xxxxx},
      archivePrefix={arXiv},
<!--       primaryClass={cs.LG}, -->
      url={https://arxiv.org/abs/xxx.xxxxx},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted
                        from theֲ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>ֲ project page.
                        You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"
                                                                                                                                                                                    href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                                                                                                                                    target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
